{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Generaliza√ß√£o em Machine Learning\n",
    "\n",
    "Neste notebook, vamos explorar um dos conceitos mais importantes em machine learning: **generaliza√ß√£o**. Veremos como os modelos se comportam quando expostos a dados que n√£o viram durante o treinamento e como evitar problemas como overfitting e underfitting.\n",
    "\n",
    "## Objetivos de Aprendizagem\n",
    "\n",
    "Ao final deste notebook, voc√™ ser√° capaz de:\n",
    "- Compreender os conceitos de overfitting, underfitting e bias-variance tradeoff\n",
    "- Implementar modelos polinomiais usando PyTorch para demonstrar estes conceitos\n",
    "- Analisar curvas de treinamento para identificar problemas de generaliza√ß√£o\n",
    "- Aplicar t√©cnicas de valida√ß√£o em datasets reais (Fashion-MNIST)\n",
    "\n",
    "## Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configura√ß√£o para gr√°ficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Dispositivo utilizado: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conceitos Fundamentais de Generaliza√ß√£o\n",
    "\n",
    "### 1.1 Defini√ß√µes\n",
    "\n",
    "**Generaliza√ß√£o** √© a capacidade de um modelo de fazer predi√ß√µes precisas em dados que n√£o foram vistos durante o treinamento.\n",
    "\n",
    "**Overfitting (Sobreajuste)**: O modelo aprende os dados de treinamento muito bem, incluindo ru√≠do e detalhes espec√≠ficos, mas falha em generalizar para novos dados.\n",
    "\n",
    "**Underfitting (Subajuste)**: O modelo √© muito simples para capturar os padr√µes subjacentes nos dados, resultando em performance ruim tanto no treinamento quanto na valida√ß√£o.\n",
    "\n",
    "### 1.2 Bias-Variance Tradeoff\n",
    "\n",
    "O erro total de um modelo pode ser decomposto em tr√™s componentes:\n",
    "\n",
    "$$\\text{Erro Total} = \\text{Bias}^2 + \\text{Vari√¢ncia} + \\text{Ru√≠do}$$\n",
    "\n",
    "- **Bias**: Erro devido a suposi√ß√µes simplificadas no algoritmo de aprendizagem\n",
    "- **Vari√¢ncia**: Erro devido √† sensibilidade a pequenas flutua√ß√µes no conjunto de treinamento\n",
    "- **Ru√≠do**: Erro irredut√≠vel nos dados\n",
    "\n",
    "![Bias-Variance Tradeoff](https://upload.wikimedia.org/wikipedia/commons/9/9f/Bias_and_variance_contributing_to_total_error.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Demonstra√ß√£o com Regress√£o Polinomial\n",
    "\n",
    "Vamos criar um exemplo pr√°tico usando regress√£o polinomial para demonstrar overfitting e underfitting.\n",
    "\n",
    "### 2.1 Classe Polinomial Customizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegressor(nn.Module):\n",
    "    \"\"\"Modelo de regress√£o polinomial usando PyTorch.\n",
    "    \n",
    "    Implementa um polin√¥mio de grau n:\n",
    "    y = a_0 + a_1*x + a_2*x^2 + ... + a_n*x^n\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, degree):\n",
    "        super(PolynomialRegressor, self).__init__()\n",
    "        self.degree = degree\n",
    "        # Coeficientes do polin√¥mio (par√¢metros aprend√≠veis)\n",
    "        self.coefficients = nn.Parameter(torch.randn(degree + 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: calcula o polin√¥mio para os valores de entrada.\"\"\"\n",
    "        # Criar matriz de pot√™ncias [1, x, x^2, x^3, ..., x^n]\n",
    "        powers = torch.stack([x.squeeze()**i for i in range(self.degree + 1)], dim=1)\n",
    "        # Calcular y = sum(a_i * x^i)\n",
    "        return torch.sum(powers * self.coefficients, dim=1, keepdim=True)\n",
    "    \n",
    "    def get_coefficients(self):\n",
    "        \"\"\"Retorna os coeficientes aprendidos.\"\"\"\n",
    "        return self.coefficients.detach().numpy()\n",
    "\n",
    "# Testar a classe\n",
    "model = PolynomialRegressor(degree=2)\n",
    "x_test = torch.linspace(-2, 2, 10).unsqueeze(1)\n",
    "y_test = model(x_test)\n",
    "print(f'Forma da entrada: {x_test.shape}')\n",
    "print(f'Forma da sa√≠da: {y_test.shape}')\n",
    "print(f'Coeficientes iniciais: {model.get_coefficients()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gera√ß√£o de Dados Sint√©ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(true_func, n_samples=100, noise_std=0.3, x_range=(-2, 2), seed=42, offset=0.0):\n",
    "    \"\"\"Gera dados com base em uma fun√ß√£o verdadeira e adiciona ru√≠do.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x = np.random.uniform(x_range[0]-offset, x_range[1]+offset, n_samples)\n",
    "    y_true = true_func(x)\n",
    "    noise = np.random.normal(0, noise_std, n_samples)\n",
    "    y_noisy = y_true + noise\n",
    "    \n",
    "    return x, y_true, y_noisy\n",
    "\n",
    "# Definir fun√ß√£o verdadeira (par√°bola)\n",
    "def true_polynomial(x):\n",
    "    return 1.5 - 2.0 * x + 0.5 * x**2 + 3.5 * x**3 \n",
    "\n",
    "x_range = (-1, 1)\n",
    "noise_std = 0.2\n",
    "test_offset = 0.5\n",
    "\n",
    "# Gerar dados\n",
    "x_train, y_train_true, y_train_noisy = generate_data(true_polynomial, n_samples=10, noise_std=noise_std, x_range=x_range)\n",
    "x_test, y_test_true, y_test_noisy = generate_data(true_polynomial, n_samples=10, noise_std=noise_std, x_range=x_range, offset=test_offset)\n",
    "\n",
    "# Converter para tensores\n",
    "x_train_tensor = torch.FloatTensor(x_train).unsqueeze(1)\n",
    "y_train_tensor = torch.FloatTensor(y_train_noisy).unsqueeze(1)\n",
    "x_test_tensor = torch.FloatTensor(x_test).unsqueeze(1)\n",
    "y_test_tensor = torch.FloatTensor(y_test_noisy).unsqueeze(1)\n",
    "\n",
    "# Visualizar os dados\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "x_smooth = np.linspace(x_range[0], x_range[1], 100)\n",
    "y_smooth = true_polynomial(x_smooth)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_train, y_train_noisy, alpha=0.6, label='Dados de Treinamento (com ru√≠do)')\n",
    "plt.plot(x_smooth, y_smooth, 'r-', label='Fun√ß√£o Verdadeira', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Dados de Treinamento')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_test, y_test_noisy, alpha=0.6, color='orange', label='Dados de Teste (com ru√≠do)')\n",
    "plt.plot(x_smooth, y_smooth, 'r-', label='Fun√ß√£o Verdadeira', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Dados de Teste')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Tamanho do conjunto de treinamento: {len(x_train)}')\n",
    "print(f'Tamanho do conjunto de teste: {len(x_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fun√ß√£o de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_polynomial_model(degree, epochs=1000, lr=0.01, verbose=False):\n",
    "    \"\"\"Treina um modelo polinomial e retorna hist√≥rico de losses.\"\"\"\n",
    "    model = PolynomialRegressor(degree)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(x_train_tensor)\n",
    "        train_loss = criterion(train_pred, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Avalia√ß√£o\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(x_test_tensor)\n",
    "            test_loss = criterion(test_pred, y_test_tensor)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 200 == 0:\n",
    "            print(f'√âpoca {epoch+1}/{epochs}, '\n",
    "                  f'Loss Treino: {train_loss:.4f}, '\n",
    "                  f'Loss Teste: {test_loss:.4f}')\n",
    "    \n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "# Testar a fun√ß√£o de treinamento\n",
    "model_test, train_losses_test, test_losses_test = train_polynomial_model(degree=3, epochs=1000, lr=0.1, verbose=True)\n",
    "print(f'\\nCoeficientes finais: {model_test.get_coefficients()}')\n",
    "print(f'Coeficientes verdadeiros: [1.5, -2.0, 0.5]')\n",
    "\n",
    "# Visualizar ajuste\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_test, y_test_noisy, alpha=0.6, label='Dados de Treinamento')\n",
    "x_plot = torch.linspace(x_range[0]-test_offset, x_range[1]+test_offset, 100).unsqueeze(1)\n",
    "with torch.no_grad():\n",
    "    y_plot = model_test(x_plot)\n",
    "plt.plot(x_plot.squeeze(), y_plot.squeeze(), 'g--', linewidth=2, label='Modelo Ajustado')\n",
    "plt.plot(x_plot.squeeze(), true_polynomial(x_plot.squeeze()), 'r-', linewidth=2, label='Fun√ß√£o Verdadeira')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Teste do Ajuste Polinomial')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compara√ß√£o de Diferentes Graus de Complexidade\n",
    "\n",
    "Agora vamos treinar modelos com diferentes graus polinomiais para demonstrar underfitting, bom ajuste e overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelos com diferentes graus\n",
    "degrees = [1, 2, 3, 5, 8]\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "print(\"Treinando modelos com diferentes graus...\")\n",
    "for degree in tqdm(degrees):\n",
    "    model, train_losses, test_losses = train_polynomial_model(degree, epochs=5000, lr=0.01)\n",
    "    models[degree] = model\n",
    "    histories[degree] = {'train': train_losses, 'test': test_losses}\n",
    "\n",
    "print(\"Treinamento conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for degree in degrees:\n",
    "    train_mse = histories[degree]['train'][-1]\n",
    "    test_mse = histories[degree]['test'][-1]\n",
    "    gap = test_mse - train_mse\n",
    "    \n",
    "    results.append({\n",
    "        'Grau': degree,\n",
    "        'MSE Treino': f'{train_mse:.4f}',\n",
    "        'MSE Teste': f'{test_mse:.4f}',\n",
    "        'Gap': f'{gap:.4f}'\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"=== AN√ÅLISE COMPARATIVA DOS MODELOS ===\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Visualizar coeficientes aprendidos\n",
    "print(\"\\n=== COEFICIENTES APRENDIDOS ===\")\n",
    "for degree in degrees:\n",
    "    coeffs = models[degree].get_coefficients()\n",
    "    print(f\"Grau {degree}: {coeffs[:min(len(coeffs), 6)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curvas de loss\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    train_losses = histories[degree]['train']\n",
    "    test_losses = histories[degree]['test']\n",
    "    \n",
    "    plt.plot(train_losses, label='Treinamento', linewidth=2)\n",
    "    plt.plot(test_losses, label='Teste', linewidth=2)\n",
    "    plt.xlabel('√âpocas')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title(f'Grau {degree}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-1, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar as predi√ß√µes dos modelos\n",
    "x_plot = torch.linspace(-2.5, 2.5, 200).unsqueeze(1)\n",
    "y_true_plot = true_polynomial(x_plot)\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # Dados de treinamento\n",
    "    plt.scatter(x_train, y_train_noisy, alpha=0.6, color='blue', s=30, label='Dados Treino')\n",
    "    plt.scatter(x_test, y_test_noisy, alpha=0.6, color='orange', s=30, label='Dados Teste')\n",
    "    \n",
    "    # Fun√ß√£o verdadeira\n",
    "    plt.plot(x_plot.squeeze(), y_true_plot, 'r-', linewidth=2, label='Fun√ß√£o Verdadeira')\n",
    "    \n",
    "    # Predi√ß√£o do modelo\n",
    "    model = models[degree]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_plot = model(x_plot)\n",
    "    \n",
    "    plt.plot(x_plot.squeeze(), y_pred_plot.squeeze(), 'g--', linewidth=3, \n",
    "             label=f'Modelo Grau {degree}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Polin√¥mio Grau {degree}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-2, 6)\n",
    "    \n",
    "    # Calcular erros finais\n",
    "    train_mse = histories[degree]['train'][-1]\n",
    "    test_mse = histories[degree]['test'][-1]\n",
    "    \n",
    "    plt.text(0.05, 0.95, f'MSE Treino: {train_mse:.3f}\\nMSE Teste: {test_mse:.3f}', \n",
    "             transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "             verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bias-Variance Tradeoff\n",
    "\n",
    "### 3.1 Fundamentos Matem√°ticos\n",
    "\n",
    "O **Bias-Variance Tradeoff** √© um conceito fundamental para entender por que modelos generalizam bem ou mal. Matematicamente, podemos decompor o erro esperado de um modelo:\n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = \\text{Bias}^2[\\hat{f}(x)] + \\text{Var}[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "Onde:\n",
    "- **$y$**: valor verdadeiro\n",
    "- **$\\hat{f}(x)$**: predi√ß√£o do modelo\n",
    "- **$\\sigma^2$**: ru√≠do irredut√≠vel nos dados\n",
    "\n",
    "#### Defini√ß√µes Matem√°ticas:\n",
    "\n",
    "1. **Bias (Vi√©s)**: Mede o quanto a predi√ß√£o m√©dia do modelo se afasta do valor verdadeiro\n",
    "   $$\\text{Bias}[\\hat{f}(x)] = E[\\hat{f}(x)] - f(x)$$\n",
    "\n",
    "2. **Vari√¢ncia**: Mede o quanto as predi√ß√µes variam entre diferentes conjuntos de treinamento\n",
    "   $$\\text{Var}[\\hat{f}(x)] = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$\n",
    "\n",
    "#### Interpreta√ß√£o Pr√°tica:\n",
    "\n",
    "- **Alto Bias, Baixa Vari√¢ncia**: Modelo consistente, mas sistematicamente errado (underfitting)\n",
    "- **Baixo Bias, Alta Vari√¢ncia**: Modelo correto em m√©dia, mas muito sens√≠vel aos dados (overfitting)\n",
    "- **Objetivo**: Encontrar o equil√≠brio que minimiza o erro total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An√°lise de Overfitting com MNIST\n",
    "\n",
    "### 4.1 Efeito do Tamanho do Dataset de Treinamento\n",
    "\n",
    "Vamos investigar como o tamanho do conjunto de treinamento afeta o overfitting usando o dataset MNIST. Iremos treinar redes neurais com diferentes quantidades de dados de treinamento e observar como isso impacta a generaliza√ß√£o.\n",
    "\n",
    "#### Hip√≥teses:\n",
    "- **Poucos dados**: Alto overfitting (grande gap entre treino e valida√ß√£o)\n",
    "- **Mais dados**: Menor overfitting e melhor generaliza√ß√£o\n",
    "- **Muitos dados**: Converg√™ncia para o limite te√≥rico de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normaliza√ß√£o espec√≠fica do MNIST\n",
    "])\n",
    "\n",
    "# Carregar datasets completos\n",
    "train_dataset_full = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Criar dataloader para teste (fixo para todas as experi√™ncias)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f'Dataset MNIST carregado!')\n",
    "print(f'Tamanho do conjunto de treinamento completo: {len(train_dataset_full)}')\n",
    "print(f'Tamanho do conjunto de teste: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Arquitetura da Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    \"\"\"Rede neural simples para classifica√ß√£o MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Testar a arquitetura\n",
    "model_test = MNISTClassifier(hidden_size=128)\n",
    "print(f'Arquitetura do modelo:')\n",
    "print(model_test)\n",
    "\n",
    "# Contar par√¢metros\n",
    "total_params = sum(p.numel() for p in model_test.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_test.parameters() if p.requires_grad)\n",
    "print(f'\\nTotal de par√¢metros: {total_params:,}')\n",
    "print(f'Par√¢metros trein√°veis: {trainable_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Fun√ß√£o de Treinamento e Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_model(train_loader, val_loader, epochs=20, lr=0.001, hidden_size=128):\n",
    "    \"\"\"Treina modelo MNIST e retorna hist√≥rico de m√©tricas.\"\"\"\n",
    "    \n",
    "    model = MNISTClassifier(hidden_size=hidden_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calcular m√©tricas da √©poca\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss_avg)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss_avg)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'√âpoca {epoch+1}/{epochs}: '\n",
    "                  f'Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.2f}% | '\n",
    "                  f'Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Experimento: Variando o Tamanho do Dataset de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir diferentes tamanhos de dataset para o experimento\n",
    "dataset_sizes = [100, 500, 10000]  # Poucos, m√©dio e muitos dados\n",
    "batch_size = 64\n",
    "val_size = 1000  # Conjunto de valida√ß√£o fixo para compara√ß√£o justa\n",
    "\n",
    "# Separar dados de valida√ß√£o (primeiros 5000 exemplos do conjunto de teste)\n",
    "val_indices = list(range(val_size))\n",
    "val_dataset = torch.utils.data.Subset(train_dataset_full, val_indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "epochs = 20\n",
    "\n",
    "# Inicializar dicion√°rios para armazenar resultados\n",
    "results_by_size = {}\n",
    "models_by_size = {}\n",
    "\n",
    "print(\"=== EXPERIMENTO: EFEITO DO TAMANHO DO DATASET ===\")\n",
    "print(f\"Conjunto de valida√ß√£o fixo: {val_size} exemplos\")\n",
    "print(f\"Tamanhos de treinamento a testar: {dataset_sizes}\")\n",
    "print(f\"√âpocas por experimento: {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar experimentos para cada tamanho de dataset\n",
    "for size in dataset_sizes:\n",
    "    print(f\"Treinando com {size} exemplos...\")\n",
    "    \n",
    "    # Criar subset do dataset de treinamento\n",
    "    # Usar √≠ndices ap√≥s os de valida√ß√£o para evitar overlap\n",
    "    train_indices = list(range(val_size, val_size + size))\n",
    "    train_subset = torch.utils.data.Subset(train_dataset_full, train_indices)\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model, history = train_mnist_model(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=epochs,\n",
    "        lr=0.001,\n",
    "        hidden_size=128\n",
    "    )\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results_by_size[size] = history\n",
    "    models_by_size[size] = model\n",
    "    \n",
    "    print(f\"Conclu√≠do para {size} exemplos\")\n",
    "\n",
    "print(\"Todos os experimentos conclu√≠dos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curvas de treinamento\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "colors = ['r', 'b', 'g']\n",
    "\n",
    "for i, size in enumerate(dataset_sizes):\n",
    "    history = results_by_size[size]\n",
    "    color = colors[i]\n",
    "    \n",
    "    # Curvas de loss\n",
    "    ax_loss = axes[0, i]\n",
    "    epochs_range = range(1, len(history['train_losses']) + 1)\n",
    "    ax_loss.plot(epochs_range, history['train_losses'], f'{color}-', linewidth=2, label='Treinamento')\n",
    "    ax_loss.plot(epochs_range, history['val_losses'], f'{color}--', linewidth=2, label='Valida√ß√£o')\n",
    "    ax_loss.set_title(f'Loss - Dataset: {size} exemplos')\n",
    "    ax_loss.set_xlabel('√âpocas')\n",
    "    ax_loss.set_ylabel('CrossEntropy Loss')\n",
    "    ax_loss.legend()\n",
    "    ax_loss.grid(True, alpha=0.3)\n",
    "    ax_loss.set_ylim([0, 2])\n",
    "    \n",
    "    # Curvas de acur√°cia\n",
    "    ax_acc = axes[1, i]\n",
    "    ax_acc.plot(epochs_range, history['train_accuracies'], f'{color}-', linewidth=2, label='Treinamento')\n",
    "    ax_acc.plot(epochs_range, history['val_accuracies'], f'{color}--', linewidth=2, label='Valida√ß√£o')\n",
    "    ax_acc.set_title(f'Acur√°cia - Dataset: {size} exemplos')\n",
    "    ax_acc.set_xlabel('√âpocas')\n",
    "    ax_acc.set_ylabel('Acur√°cia (%)')\n",
    "    ax_acc.legend()\n",
    "    ax_acc.grid(True, alpha=0.3)\n",
    "    ax_acc.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 An√°lise dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar resultados finais\n",
    "print(\"=== AN√ÅLISE DE OVERFITTING POR TAMANHO DO DATASET ===\\\\n\")\n",
    "\n",
    "analysis_results = []\n",
    "for size in dataset_sizes:\n",
    "    history = results_by_size[size]\n",
    "    \n",
    "    # M√©tricas finais (√∫ltima √©poca)\n",
    "    final_train_loss = history['train_losses'][-1]\n",
    "    final_val_loss = history['val_losses'][-1]\n",
    "    final_train_acc = history['train_accuracies'][-1]\n",
    "    final_val_acc = history['val_accuracies'][-1]\n",
    "    \n",
    "    # Gap entre treino e valida√ß√£o (indicador de overfitting)\n",
    "    loss_gap = final_val_loss - final_train_loss\n",
    "    acc_gap = final_train_acc - final_val_acc\n",
    "    \n",
    "    analysis_results.append({\n",
    "        'Dataset Size': size,\n",
    "        'Train Loss': f'{final_train_loss:.4f}',\n",
    "        'Val Loss': f'{final_val_loss:.4f}',\n",
    "        'Loss Gap': f'{loss_gap:.4f}',\n",
    "        'Train Acc': f'{final_train_acc:.2f}%',\n",
    "        'Val Acc': f'{final_val_acc:.2f}%',\n",
    "        'Acc Gap': f'{acc_gap:.2f}%'\n",
    "    })\n",
    "    \n",
    "    print(f\"üìà Dataset com {size} exemplos:\")\n",
    "    print(f\"   Loss: Treino {final_train_loss:.4f} | Valida√ß√£o {final_val_loss:.4f} | Gap {loss_gap:.4f}\")\n",
    "    print(f\"   Acc:  Treino {final_train_acc:.2f}% | Valida√ß√£o {final_val_acc:.2f}% | Gap {acc_gap:.2f}%\")\n",
    "    print()\n",
    "\n",
    "# Criar tabela resumo\n",
    "df_analysis = pd.DataFrame(analysis_results)\n",
    "print(\"üìã TABELA RESUMO:\")\n",
    "print(df_analysis.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Visualiza√ß√µes Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de barras comparativo do gap de overfitting\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Extrair dados para os gr√°ficos\n",
    "dataset_sizes_str = [str(size) for size in dataset_sizes]\n",
    "loss_gaps = []\n",
    "acc_gaps = []\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    history = results_by_size[size]\n",
    "    loss_gap = history['val_losses'][-1] - history['train_losses'][-1]\n",
    "    acc_gap = history['train_accuracies'][-1] - history['val_accuracies'][-1]\n",
    "    loss_gaps.append(loss_gap)\n",
    "    acc_gaps.append(acc_gap)\n",
    "\n",
    "# Gr√°fico de gap de loss\n",
    "bars1 = ax1.bar(dataset_sizes_str, loss_gaps, color=['lightcoral', 'lightblue', 'lightgreen'], \n",
    "                edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax1.set_title('Gap de Loss (Val - Train)\\n(Maior = Mais Overfitting)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Tamanho do Dataset de Treinamento')\n",
    "ax1.set_ylabel('Gap de Loss')\n",
    "ax1.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, value in zip(bars1, loss_gaps):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gr√°fico de gap de acur√°cia\n",
    "bars2 = ax2.bar(dataset_sizes_str, acc_gaps, color=['lightcoral', 'lightblue', 'lightgreen'], \n",
    "                edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax2.set_title('Gap de Acur√°cia (Train - Val)\\n(Maior = Mais Overfitting)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Tamanho do Dataset de Treinamento')\n",
    "ax2.set_ylabel('Gap de Acur√°cia (%)')\n",
    "ax2.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, value in zip(bars2, acc_gaps):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
    "             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Conclus√µes do Experimento\n",
    "\n",
    "#### Implica√ß√µes Pr√°ticas:\n",
    "\n",
    "- **Mais dados = Melhor generaliza√ß√£o**: O aumento do tamanho do dataset reduziu consistentemente o overfitting\n",
    "- **Bias-Variance Tradeoff**: Datasets maiores reduzem a vari√¢ncia do modelo\n",
    "- **Capacidade do Modelo**: A mesma arquitetura se beneficia diferentemente dependendo da quantidade de dados\n",
    "\n",
    "#### Insights Te√≥ricos:\n",
    "\n",
    "- O **ru√≠do nos dados** tem menos impacto quando h√° mais exemplos diversos\n",
    "- A **complexidade do modelo** deve ser ajustada de acordo com a quantidade de dados dispon√≠veis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
